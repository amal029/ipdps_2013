\section{Experiments and results}
\label{sec:experiments-results}

% To determine how good our framework is, we compare it against an
% adaptation of the well known heterogeneous bin packing solution as
% discussed by Teodor et al.~\cite{tcra11}. They adapt the well-known
% \textit{Best First Decreasing (BFD)} heuristic, which works only for
% homogeneous bins, to form the \textit{Adapted BFD (A-BFD)} heuristic
% which works for heterogeneous bins. We have chosen to compare our
% framework against a bin-packing heuristic, because it is a well known
% fact that the optimal solution for a partitioning problem is NP hard,
% one cannot find optimal solutions for large processing
% architectures. Heuristic bin packing solutions have given good results
% in the general case~\cite{ecof78}. There exists no other heuristics (to the
% best of our knowledge) other than A-BFD which is able to model both vector and
% scalar processing elements in a given architecture. Hence, comparing with
% A-BFD allows us to gauge the effectiveness of our algorithm against a standard
% technique.


\subsection{Our implementation}
\label{sec:our-implementation}

We use the Metis~\cite{gkar95} graph partitioning library in the implementation
of our partitioning algorithm. We are not tied to Metis and any other graph
partitioner such as Zoltan~\cite{kdev09} or Scotch~\cite{cche08} can be
used for implementing our algorithm.

We now describe how we used Metis to implement the graph partitioning.
The resource graph is represented in the Metis graph format. We
represent the PEs' capabilities as constraints of the nodes and the
links' bandwidths as communication weight on the edges. We then
construct our clustered structure (Figure~\ref{fig:res}) by asking for a
2-way partition at each level of the $log_2|V_r|$ height. Metis
partitions the graph by load balancing the constraints and performing a
minimum edge cut.  In partitioning the filter graph, we need to distribute
the constraints on to the available partitions such that capability and requirement
is balanced. Metis offers the ability
to load balance multiple constraints on to different partitions based on
the metric \mbox{`tp-weight'}. This metric basically represents the ratio between
a given type of constraint across the different partitions. We calculate the ratios
between the capabilities of different partitions and represent them as this metric
in order to load balance on to the available partitions.

%~ \subsection{The Simulate annealing approach}
%~ \label{sec:simul-anne-appr}
%~
%~ We compare our heuristic framework with a well establish meta-heuristic
%~ called \textit{Simulated Annealing} (SA) as described in~\cite{horsi06}
%~ for resource allocation. We had to modify the SA approach described
%~ in~\cite{horsi06} to include SIMD parallelism. This was done by simply
%~ modifying the objective function to the one described in~(\ref{eq:2}).

% \subsection{Random Graph Generator}
% \label{sec:rand-graph-generator}

% \subsection{Heterogeneous Bin Packing}

% Let $\mathcal{I}$ be the items to be accommodated into the bins and let
% $\mathcal{K}$ be the set of bins available.  From the standpoint of the
% mapping problem, $\mathcal{I}$ refers to the set of
% \mbox{application-filters ($|V_t|$)} and $\mathcal{K}$ refers to the PEs
% ($|V_r|$). Similar to the Knapsack problem~\cite{sski08}, by which A-BFD
% is inspired, each element $i \in \mathcal{I},\ \mathcal{K}$ has two
% constraints on them represented by \mbox{$c_i$ (cost)}, which models
% $R^i_0$ and $V_i$ (volume), which models $R^i_1$, capabilities of the
% resource graph, respectively.

% \textit{A-BFD} proceeds to sort $\mathcal{I}$ according to
% non-increasing order of their volume and sorts $\mathcal{K}$ according
% to non-increasing order of the ratio $c_i/V_i$. Then, it proceeds to
% allocate items from $\mathcal{I}$ into best bins $b \in \mathcal{S}$. A
% ``best" bin, i.e., the bin with maximum free space, is defined as the
% bin volume minus the sum of volumes of the items loaded into
% it.

% The post pass in \textit{A-BFD} chooses every bin that has atleast one
% item allocated to it and tries to find an empty bin, that has a higher
% or equal volume than the allocated volume on the chosen bin but also has
% a lower cost. If it finds such an empty bin, then it transfers all the
% items allocated to the chosen bin to the newly found empty bin which is
% cheaper. One of the main advantages of\ \textit{A-BFD} is that it is
% very fast with a best case complexity of $O(N_\mathcal{I})$ without the
% post pass, where $N_\mathcal{I}$ is the number of items (number of filters
% $|V_t|$ in the filter graph $G_t$). Including the post pass, the best case
% complexity becomes $O(N_\mathcal{I} + N_\mathcal{K})$ where
% $N_\mathcal{K}$ is the number of bins (number of PEs $|V_r|$ in the
% resource graph $G_r$).

\subsection{The experimental set-up}
\label{sec:experimental-setup}

The experimental set-up consists of the resource graph generation and the
filter graph generation. Herein, we describe the two set-ups.

\subsubsection{The resource graph set-up}
\label{sec:resource-graph-setup}

The experimental set up consists of the following.

\begin{enumerate}

\item An interconnection network with \numtplgynodes
  nodes. \numtplgynodes varies from 64 to 4096 PEs. A node can be just a
  multi-core CPU or a multi-core CPU with an attached GPU.

\item A set of \gpunum GPUs where \gpunum is at most \numtplgynodes. The
  GPUs are connected in the network at locations, chosen randomly in the
  normal distribution of 25\% to 75\% of $|V_r|$.

\item A set $\veclenset = \{G_1, G_2, G_3, ... G_{|\veclenset|}\}$
  Every GPU in this experiment has a vector
  length of $G_i$ where $G_i$ is sampled randomly from the set
  \veclenset. The elements of set \veclenset are chosen from a normal
  distribution ranging from: 10000 to 100000.

\item A set $\corenumset = \{C_1, C_2, C_3, ... C_{|\corenumset|}\}$
  Every CPU in this experiment has $C_i$
  cores where $C_i$ is sampled randomly from the set \corenumset.

\item A set $\mipsset = \{M_1, M_2, M_3, ... M_{|\mipsset|}\}$
   Every $C_i \in \corenumset$ and GPU in this
  experiment has a MIPS count of $M_i$ where $M_i$ is sampled randomly
  from the set \mipsset. The elements of set \mipsset are chosen from a
  normal distribution ranging from: 1000 to 100000.

\item A set $\bw = \{B_1, B_2, B_3, ... B_{|B|}\}$
  Every $|E_r|$ edge in this experiment has a bandwidth of $B_i$ in MB/s where $B_i$
  is sampled randomly from a normal distribution ranging from: 100 to 100000

\end{enumerate}

For given values of \numtplgynodes, \gpunum, \veclenset, \corenumset, \mipsset
and \bw and a given application, let the $k$-th \ul{trial} be
defined as one execution of the following sequence of steps.

\begin{itemize}

\item For each GPU $G_i$, sample \veclenset and \mipsset randomly to
  determine its vector length $V_i$ and MIPS count $M_i$. \label{i1}

\item For each CPU $P_i$, sample \corenumset randomly to determine the
  number of cores $C_i$ in the processor $P_i$.~\label{i2}

\item For each core $C_i$ in the processor $P_i$ sample $V_i$ and $M_i$
  randomly from set \veclenset and \mipsset.

\item Use our framework to extract data and filter parallelism that is
  best utilizable by the heterogeneity created by parameters in items 1,
  2, and 3 above. Determine the execution time $\mathbb{P}$.

\end{itemize}

An experiment, \expt(\numtplgynodes, \gpunum, \veclenset, \corenumset
\mipsset, \bw), consists of conducting enough of the above trials so that
width of the 95\% confidence interval on the average value of
$\mathbb{P}$ is less than 10\% of the average value. This results in a variable
number of trials with different experimental set-ups. Note that two trials
differ from each other only in the seed for the random number generator.  This
reduces the dependence of our results on a lucky sequence of numbers from the
random number generator.

\subsubsection{Random application graph generation}
\label{sec:filter-graph-setup}

We built a random graph generator to test our partitioning methodology
rigorously. The random graph generator takes as input the following
parameters:

\begin{itemize}
\item Number of nodes ($n$) - Total number of nodes to be present in the
  filter graph
\item Indegree ($i$) - Average indegree of every vertex
\item Outdegree ($o$) - Average outdegree of every vertex
\item Communication to Computation Ratio - CCR ($c$) - It is the ratio
  of the average communication cost of an out-edge the average
  computation cost of the vertex itself. If a application graph's $c$ is low,
  then it can be called a computation intensive application and if it is greater
  than 1 it can be called a communication intensive application
\item Structure of the graph ($\alpha$) - We generate the height of the
  graph based on $\alpha$ as $\frac{\sqrt{n}}{\alpha}$. This implies the
  width of the graph becomes $\sqrt{n} \times {\alpha}$. Higher values of
  $\alpha$ give wider graphs, which means the graph has more inherent
  task parallelism, while lower values give taller graphs, which means
  the graph is inherently serial
\item Beta ($\beta$) - We use this parameter to decide if an actor in
  the filter graph is CPU intensive or GPU intensive. Smaller values of
  $\beta$ makes actors CPU intensive (by making first constraint larger
  than the second), while larger values make it GPU intensive (by making
  the second constraint larger than the first). This essentially means
  that smaller values of $\beta$ creates filters, which require lot more
  non-vectorized units, whereas larger values would result in filters
  with larger vector requirement.
\item Skewedness factor ($\gamma$) - This parameter dictates how
  computation is spread across the graph. Smaller values of $\gamma$
  give uniformly distributed values for the constraints of the actors
  while larger values produces skewed graphs
\end{itemize}

\noindent For our experiments, random graphs are generated by choosing values for
the input parameters from the following sets:

\begin{itemize}
\item $\chi_{n} = \{128, 256, 512, 1024, 4096, 8192, 16384\}$
%\item $\chi_{i} = \{1\}$
\item $\chi_{o} = \{2, 4, 8\}$
\item $\chi_{c} = \{0.0001, 0.001, 0.01, 0.1, 1\}$
\item $\chi_{\alpha} = \{0.1, 1.0, 10.0\}$
\item $\chi_{\beta} = \{5, 25, 50, 75, 95\}$
\item $\chi_{\gamma} = \{5, 25, 50, 75, 95\}$
\end{itemize}

{\setlength{\parindent}{0cm}
We generated one graph per combination for a total of 7875 application
graphs. Since the random graph generator has a variety of inputs and these
inputs are filled in from a large set of possible values, a diverse set of
application graphs are generated with various characteristics. Experiments
based on such diverse set of application graphs prevents biasing towards a
particular partitioning algorithm. It also allows us to evaluate how HMAP
heuristic performs for different category of application graphs.
}

% We chose 5 applications from the HPC arena: binomial option pricing (a
% financial derivatives application), 2-dimensional convolution (for image
% processing), Gram Schmidt linear algebra kernel, 2-dimensional
% Gauss-Seidel stencil computations, and finally our motivating example
% itself the 2-dimensional Jacobi stencil computation. Next for these 5
% applications, we varied the vector strip from 10 to 50, which resulted
% in graphs varying from around 50 to 5000 nodes and with 23 to 12,000
% edges. For example, given a filter node with a vector length requirement
% ($T^i_1$) of 30,000 elements, a vector strip of 10 means dividing the
% total vector requirement by 10, which results in 10 nodes each requiring
% 3000 vector elements. Similarly, the LLVM instruction count ($T^i_0$)
% for each node varies depending upon the application at hand.

% A detailed description of the applications and their features is shown
% in Table~\ref{tab:1}. In general the vector requirement of the
% applications in our benchmark suite varies from 1000 to 1 Million
% elements. The LLVM instruction count varies from around 1 to 0.3
% billion. The edge weights depicting the amount of data transfer on the
% other hand varies from 3000 bytes to almost 4.8 Mega byte.

% \begin{small}
%   \begin{table}[h!]
%     \centering
%     \resizebox{!}{38mm}{
%       \begin{tabular}{|c|c|c|c|}
%         \hline
%         \textbf{Application} & \textbf{Vector strip} & $|V_t|$ & $|E_t|$ \\
%         \hline
%         \multirow{5}{*}{Binomial option pricing} & 10 & 82 & 206 \\
%         & 20 & 102 & 306 \\
%         & 30 & 122 & 406 \\
%         & 40 & 142 & 506 \\
%         & 50 & 162 & 606 \\
%         \hline
%         \multirow{5}{*}{Convolution} & 10 & 79 & 143 \\
%         & 20 & 89 & 173 \\
%         & 30 & 99 & 203 \\
%         & 40 & 109 & 233 \\
%         & 50 & 119 & 263 \\
%         \hline
%         \multirow{5}{*}{Gram Schmidt} & 10 & 228 & 443 \\
%         & 20 & 838 & 1653 \\
%         & 30 & 1848 & 3663 \\
%         & 40 & 3258 & 6473 \\
%         & 50 & 5068 & 10083\\
%         \hline
%         \multirow{5}{*}{Gauss-Seidel} & 10 & 227 & 531 \\
%         & 20 & 837 & 2041 \\
%         & 30 & 1847 & 4551 \\
%         & 40 & 3257 & 8061 \\
%         & 50 & 5067 & 12571\\
%         \hline
%         \multirow{5}{*}{Jacobi} & 10 & 48 & 130 \\
%         & 20 & 78 & 240 \\
%         & 30 & 108 & 350 \\
%         & 40 & 138 & 460 \\
%         & 50 & 168 & 570\\
%         \hline
%       \end{tabular}
%     }
%     \caption{The filter graph set-up}
%     \label{tab:1}
%   \end{table}
% \end{small}

% % We have done experiments for several different sets of parameters. In
% % this paper we show the results for the parameters enumerated in Table
% % \ref{parametr_tbl}.

% \begin{figure*}[t!]
%   \centering
%   \subfigure[Binomial Option Pricing]{
%     \includegraphics[angle=0, scale=0.7]{./figures/bin_surface}
%     \label{fig:bin1ho}
%   }
%   \subfigure[2 Dimensional Convolution]{
%     \includegraphics[angle=0, scale=0.7]{./figures/conv_surface}
%     \label{fig:conv1ho}
%   }
%   \subfigure[Gram Schmidt linear-algebra kernel]{
%     \includegraphics[angle=0, scale=0.7]{./figures/gram_surface}
%     \label{fig:gram1ho}
%   }
%   %~ \subfigure[2 Dimensional Seidel stencil computation]{
%     %~ \includegraphics[angle=0, scale=0.72]{./figures/sei_surface}
%     %~ \label{fig:sei1ho}
%   %~ }
%   \subfigure[2 Dimensional Jacobi stencil computation]{
%     \includegraphics[angle=0, scale=0.7]{./figures/jac_surface}
%     \label{fig:jacl1ho}
%   }
%   \caption{Comparison of execution times of ``Our Framework" and
%     ``Heterogeneous Bin Packing"}
%   \label{fig:ho}
% \end{figure*}

% \begin{figure*}[t!]
%   \centering
%   \subfigure[Binomial Option Pricing]{
%     \includegraphics[angle=0, scale=0.55]{./figures/bin_32-64_surface}
%     \label{fig:bin1comm}
%   }
%   \subfigure[2 Dimensional Jacobi stencil computation]{
%     \includegraphics[angle=0, scale=0.55]{./figures/jac_32-64_surface}
%     \label{fig:jac1comm}
%   }
%   \caption{Choosing the proper tile size}
%   \label{fig:comm}
% \end{figure*}

\subsection{Experimental Results}
\label{sec:results-1}

\begin{figure*}[ht!]
  \centering
  \subfigure[CPU intensive graphs
    ($\beta-5,25,50$) ]
  {
    \includegraphics[angle=0, scale=0.44]{./figures/cpu}
    \label{fig:cpu}
  }
  \subfigure[GPU intensive graphs ($\beta-50,75,95$)]{
    \includegraphics[angle=0, scale=0.44]{./figures/gpu}
    \label{fig:gpu}
  }
  \subfigure[Serial applications
  ($\alpha-1,0.1$ $\beta-50$)]{
    \includegraphics[angle=0, scale=0.44]{./figures/serial}
    \label{fig:serial}
  }
  \subfigure[Task parallel applications
  ($\alpha-10,1$ $\beta-50$)]{
    \includegraphics[angle=0, scale=0.44]{./figures/parallel}
    \label{fig:parallel}
  }
  \subfigure[Comm. intensive graphs
  ($\alpha-1$ $\beta-50$ $c-1$)]{
    \includegraphics[angle=0, scale=0.44]{./figures/comm}
    \label{fig:comm}
  }
  \subfigure[Overall]{
    \includegraphics[angle=0, scale=0.44]{./figures/overall}
    \label{fig:overall}
  }
  \caption{Experimental Results\\ \footnotesize{Each graph shows the
  application period $\mathbb{P}$ for a given class of application, on
  architecture consisting of various $|V_r|$ nodes}}
  \label{fig:exp-res}
\end{figure*}

The experimental set-up consists of a dual socket system consisting of
Intel Xeon E5620 CPU running at 2.4Ghz with 24GB DDR3 RAM. The system
is running Linux kernel ver 3.0.40-1. The HMAP heuristic was compiled
using GCC version 4.7.1 with `-O3' optimization flag.

We ran over 100,000 experiments, using the random application graphs and
our HMAP heuristic and K-way partitioning for seven resource architectures.
Overall it took us 74 sec on average for the biggest architecture of 4096
nodes to determine a partition of the application graph onto the given
hetrogeneous architecture, which is 50 sec more than Metis.
The results comparing K-way partitioning using Metis~\cite{gkar95} and our HMAP
framework are shown in Figure~\ref{fig:exp-res}. Once the partitioning of the
application graph for the given architecture is determined, we calculate the
application period using Equation~(\ref{eq:2}).

The results are divided into five graphs, each representing a common
class (based on section~\ref{sec:filter-graph-setup}) of application
graphs. The final graph presents the average application period for each architecture.

Figures~\ref{fig:cpu} and~\ref{fig:gpu} represent application graphs
that consists of proportionally larger non-vectorized and vectorized
requirements, respectively. The application graphs with values of $\beta = 5,\ 25,\
\mathrm{and}\ 50$ are more non-vectorized and require a higher MIPS
count. Whereas the application graphs with $\beta = 50,\ 75,\ \mathrm{and}\ 95$ are
those that require a higher vector count. On resource architecture of size 64
nodes, Metis results in an application partition with the period being
$1.2\times$ faster than the one obtained by our scheme. But, for the
rest of the architectures we outperform the partition obtained by
standard Metis. For example, in the case of 4096 node architecture, the
resultant application period from our HMAP scheme is $3\times$ faster
compared to the one obtained via Metis.

% performs 1.2x faster than our heuristic but on all bigger sizes we start
% to perform better and for the largest one of 4096 nodes we perform over
% 3x faster than k-way partioning alone.

In the case of Figures~\ref{fig:serial} and~\ref{fig:parallel}, which
represent more serial and task-parallel application graphs, respectively
we notice a similar trend. Metis outperforms the HMAP scheme for smaller
architectures but HMAP outperforms the partition obtained via Metis in
all other cases.

% that belong to $\alpha-1,0.1 \beta-50$(serial) and $\alpha-10,1
% \beta-50$ (parallel) respectively.  are tall consisting of more filters
% that can only be executed serially and fat consisting of more task
% parallel filters, the trend continues and we are able to provide better
% latencies in the case of larger archtiectures when compared with metis
% alone.

We attribute such behavior to the following; for smaller architectures,
the variation (in terms of MIPS and vector lengths) in the underlying
physical architecture can be easily captured in standard Metis using its
so called constraints `tp-weights'. For larger architectures with a very
large number of nodes as in our HPC case, these variations in the
underlying architecture cannot be expressed in Metis in the form of
constraints. We alleviate this inexpressibility by clustering the large
number of heterogeneous nodes into a smaller number of homogeneous
clusters as shown in Figure~\ref{fig:res}.

% the constraints (tp-weights) are enough to show the difference between
% machines to metis. Whereas, when the architecture becomes big, the
% relative difference between two partitions become so small that metis is
% unable to tell them apart. The problem becomes one of unexpressability.
% Our framework alleviates this by asking for a very small number of
% partitions(usually in the range of 2 to 4) at every stage.

Figure~\ref{fig:comm} shows the result of partitioning communication
intensive application graphs on the same set of heterogeneous HPC
physical architectures. We perform significantly better in comparison to
Metis for larger archtiectures. This is due to our clustering approach
that ensures topology nodes that communicate with high bandwidths are
combined together at each level. Moreover, Metis is unaware of the
communication between nodes in the topology as it only performs a
min-cut when partitioning the application graphs. Our framework on the
other hand, takes into account the communication in the topology and
indirectly matches heavy edges from the filter graph onto high bandwidth
edges in the topology graph.

Figure~\ref{fig:overall} shows the average application period based
on all the input graphs. HMAP performs better than standard metis on all
architectures bigger than 64 nodes with an average speedup of more than
$1.5\times$ and for the largest resource architecture of 4096 nodes, HMAP
performs $3\times$ better than standard metis.

Finally, HMAP does not perform as well on physical topologies wherein
the clustering of the resource graph does not result in clusters with
equal number of nodes. For example, consider the 2048 node cluster,
which is a $64\times32$ 2D mesh. In such cases, clustering the given
topology into virtual clusters (see Figure~\ref{fig:res}) results in
clusters without equal number of nodes. Improving performance, further
still, in such topologies remains an open question, which we plan to
deal with in the future.

% We also observed that metis performs better than our framework in the
% case of rectangular topologies. We believe this is because rectangular
% topologies are harder to collate into homogenous clusters as there are
% straddlers in non-square architectures.

% Finally, in Figure~\ref{fig:overall} we see that we are able to scale
% better when compared with metis. In smaller architecture sizes metis is
% able to produce good mapping, whereas on larger architecture sizes we
% are able to perform over several magnitudes better in comparison.


% The results for the experiments with the above experimental set-up
% comparing our approach with the aforementioned heterogeneous bin packing
% approach is shown in Figure~\ref{fig:ho}. Our approach performs better
% compared to heterogeneous bin packing for all the applications. The
% graphs in Figure~\ref{fig:ho} consists of two surfaces layered on top of
% each other. In these graphs the larger the value, the worse the latency
% and hence the performance of the application. % For all graphs, the
% % surface representing the heterogeneous bin-packing solution is always
% % layered atop our results, which clearly show the superiority of our
% % approach. Note that the z-axis in all the graph in Figure~\ref{fig:ho}
% % are in logarithmic scale with a base of 10. This in turn implies that
% % our results are almost an order of magnitude better than those
% % obtained by the heterogeneous bin packing heuristic.

% For Seidel, the results for bin-packing could not be obtained, because
% the required vector element length was larger than the available
% capability of the underlying hardware. Our approach is able to deal with
% such a situation, by fitting the required vector length to the
% capability and then running the rest in an iterative manner.

% The major differences between the two algorithms are provided in
% Table~\ref{tab:2}. The \textbf{Max latency} and \textbf{Min latency}
% columns give the maximum and the minimum latencies for bin packing and
% our framework, respectively. The final column gives the number of data
% points that our framework is better compared to heterogeneous bin
% packing. As we can see from Table~\ref{tab:2}, our framework out
% performs the heterogeneous bin-packing on all the applications.

% \begin{small}
%   \begin{table*}[t!]
%     \centering
%     \resizebox{!}{11mm}{
%       \begin{tabular}{|c|c|c|c|c|c|}
%         \hline
%         \textbf{Application} &
%         \multicolumn{2}{|c|}{\textbf{Max latency (sec)}} &
%         \multicolumn{2}{|c|}{\textbf{Min latency (sec)}} &
%         \textbf{Better (\%)} \\
%         \hline
%         & {HBP} & {Our framework} &
%         {HBP} & {Our framework} & HBP vs Our framework\\
%         \hline
%         Binomial option pricing & $1.35e+8$ & $5.27e+7$ & 77356.9 & 1 & 100 \\
%         \hline
%         Convolution & 489065 & 78.52 & 53.64 & 16.26 & 94\\
%         \hline
%         Gram Schmidt & 18984.5 & 177.97 & 2947.65 & 1.20 & 94\\
%         \hline
%         Gauss-Seidel & N/A & 3635.62 & N/A & 9.67 & 100\\
%         \hline
%         Jacobi & 13919.2 & 39.89 & 1.69 & 0.97 & 92\\
%         \hline
%       \end{tabular}
%     }
%     \caption{Major statistics comparing heterogeneous bin packing and our framework}
%     \label{tab:2}
%   \end{table*}
% \end{small}

% There are a number of reasons for the large differences seen in
% Figure~\ref{fig:ho}.

% \begin{itemize}

% \item The bin packing solution gives priority to volume, which means
%   vectors are packed into a single large vector processor, while giving
%   very little consideration to the instruction count requirement.

% \item The bin packing heuristic does not consider placing data-stores in
%   the correct locations, in fact data-stores do not play a part in the
%   whole process at all and the same can be said about communication.

% \item Bin-packing ignores communication unlike our framework.

% \end{itemize}

% In order to show how communication can have a significant
% effect on the application design and parallelism potential, in the next
% section we perform design space exploration with the effect
% of communication as the main objective.

%~ Since communication does not play a part in the heuristic bin-packing
%~ solution, we ignored the communication values when comparing with bin
%~ packing. However, in order to show how communication can have a significant
%~ effect onto the application design and parallelism potential, in the next
%~ section we perform design space exploration with the effect
%~ of communication as the main objective.

% \subsection{Affects of varying communication}
% \label{sec:arch-expl-with}

% % In the previous section we compared our results with heterogeneous bin
% % packing, without any regards for the communication. This was done by
% % keeping the bandwidth of the resource graph links high enough to not
% % have any affect on the overall application latency. We purposely did
% % this, in order to make a fair comparison with bin packing since it does
% % not consider communication when partitioning the nodes in the filter
% % graph.

% In this section we vary the bandwidth (weights on the resource graph
% edges, $E^C$) to explore best vector tile sizes for the applications.
% For experimental purposes, we use a 2D tiled mesh of 16 $\times$ 16 PEs
% (total of 256 PEs). The bandwidth of the links is randomly selected from
% a normal distribution between 32-64 MB/s for each edge. In this set-up,
% a GPU-CPU combination is placed at every fourth corner in the tiled
% mesh. Finally, every CPU core in this set-up runs at 2GHz and has a
% vector capacity of 64, i.e., 64 floating point elements can be processed
% at once. Similarly, all GPUs run at 700MHz and can perform 4096 floating
% point operations in a SIMD fashion. The application set is varied in the
% exact same manner as the previous experimental set-up.


% Two of the most interesting results that we obtained for the above
% experimental set-up is shown in
% Figure~\ref{fig:comm}. Figure~\ref{fig:bin1comm}, gives the varying
% latency of execution for the different vector strip sizes for the
% binomial option pricing application. As we can see, with increasing
% vector strip sizes the latency of execution reduces. In our experiments
% we found that a vector strip size of 50 is best suited for this example
% and gave the best performance. Moreover, we also found that from the 256
% PEs, this example only utilized 46 PEs and that too mostly GPUs.  Jacobi
% on the other hand performs best for a vector strip size of
% 20. Increasing the vector strip size after that has an adverse affect on
% the resulting partitioning as the communication overheads are far
% greater than the computation potential. Finally, in both these
% applications the placement of the data stores plays an important role,
% since there is a single data store, which needs to be accessed by all
% the nodes split due to vector tiling, the communication overhead
% increases if the store is not placed on a correct PE: usually a PE with
% high bandwidth connection to the PE processing the vector statement.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "bare_conf"
%%% End:

\section{Experiments and results}
\label{sec:experiments-results}

To determine how good our framework is, we compare it against an
adaptation of the well known heterogeneous bin packing solution as
discussed by Teodor et al.~\cite{tcra11}. They adapt the well-known
\textit{Best First Decreasing (BFD)} heuristic, which works only for
homogeneous bins, to form the \textit{Adapted BFD (A-BFD)} heuristic
which works for heterogeneous bins. We have chosen to compare our
framework against a bin-packing heuristic, because it is a well known
fact that the optimal solution for a partitioning problem is NP hard,
one cannot find optimal solutions for large processing
architectures. Heuristic bin packing solutions have given good results
in the general case~\cite{ecof78}. Comparing with the heterogeneous bin
packing (HBP) heuristic allows us to gauge the effectiveness of our algorithm
against a standard effective technique.

\subsection{Our implementation}
\label{sec:our-implementation}

We use the Metis~\cite{gkar95} graph partitioning library to implement
our partitioning algorithm. We are not tied to Metis and any other graph
partitioner such as Zoltan~\cite{kdev09} or Scotch~\cite{cche08} can be
used for implementing our algorithm. Herein, we describe how we used
Metis to implement the graph partitioning.

The resource graph is represented in the Metis graph format. We
represent the PEs capabilities as constraints of the nodes and the
link's bandwidths as communication weight on the edges. We then
construct our clustered structure (Figure~\ref{fig:res}) by asking for a
2-way partition at each level of the $log_2|V_r|$ height. Metis
partitions the graph by load balancing the constraints and performing a
minimum edge cut.  In partitioning the task graph, we need to balance
the constraints on to the available partitions. Metis offers the ability
to load balance multiple constraints on to different partitions based on
the metric \mbox{`tp-weight'}. We calculate the ratios between the
capabilities of different partitions and represent them as this metric
in order to load balance on to the available partitions. 


% \subsection{Heterogeneous Bin Packing}

% Let $\mathcal{I}$ be the items to be accommodated into the bins and let
% $\mathcal{K}$ be the set of bins available.  From the standpoint of the
% mapping problem, $\mathcal{I}$ refers to the set of
% \mbox{application-tasks ($|V_t|$)} and $\mathcal{K}$ refers to the PEs
% ($|V_r|$). Similar to the Knapsack problem~\cite{sski08}, by which A-BFD
% is inspired, each element $i \in \mathcal{I},\ \mathcal{K}$ has two
% constraints on them represented by \mbox{$c_i$ (cost)}, which models
% $R^i_0$ and $V_i$ (volume), which models $R^i_1$, capabilities of the
% resource graph, respectively.

% \textit{A-BFD} proceeds to sort $\mathcal{I}$ according to
% non-increasing order of their volume and sorts $\mathcal{K}$ according
% to non-increasing order of the ratio $c_i/V_i$. Then, it proceeds to
% allocate items from $\mathcal{I}$ into best bins $b \in \mathcal{S}$. A
% ``best" bin, i.e., the bin with maximum free space, is defined as the
% bin volume minus the sum of volumes of the items loaded into
% it.

% The post pass in \textit{A-BFD} chooses every bin that has atleast one
% item allocated to it and tries to find an empty bin, that has a higher
% or equal volume than the allocated volume on the chosen bin but also has
% a lower cost. If it finds such an empty bin, then it transfers all the
% items allocated to the chosen bin to the newly found empty bin which is
% cheaper. One of the main advantages of\ \textit{A-BFD} is that it is
% very fast with a best case complexity of $O(N_\mathcal{I})$ without the
% post pass, where $N_\mathcal{I}$ is the number of items (number of tasks
% $|V_t|$ in the task graph $G_t$). Including the post pass, the best case
% complexity becomes $O(N_\mathcal{I} + N_\mathcal{K})$ where
% $N_\mathcal{K}$ is the number of bins (number of PEs $|V_r|$ in the
% resource graph $G_r$).

\subsection{The experimental set-up}
\label{sec:experimental-setup}

The experimental set-up consists of the resource graph generation and the
task graph generation. Herein, we describe the two set-ups.

\subsubsection{The resource graph set-up}
\label{sec:resource-graph-setup}

The experimental set up consists of the following.

\begin{enumerate}

\item An interconnection network with \numtplgynodes
  nodes. \numtplgynodes vary from 2 to 121 PEs. A node could be just a
  multi-core CPU or a multi-core CPU with a GPU attached to it.

\item A set of \gpunum GPUs where \gpunum is at most \numtplgynodes. The
  GPUs are connected in the network at pre-determined locations, chosen
  randomly in the normal distribution of 25\% to 75\% of $|V_r|$.

\item A set $\veclenset = \{G_1, G_2, G_3, ... G_{|\veclenset|}\}$ where
  $G_i$ is a power of 2.  Every GPU in this experiment has a vector
  length of $G_i$ where $G_i$ is sampled randomly from the set
  \veclenset. The elements of set \veclenset are chosen from a normal
  distribution ranging from: 1024 to 16284.

\item A set $\corenumset = \{C_1, C_2, C_3, ... C_{|\corenumset|}\}$
  where $C_i$ is a power of 2.  Every CPU in this experiment has $C_i$
  cores where $C_i$ is sampled randomly from the set \corenumset.

\item A set $\mipsset = \{M_1, M_2, M_3, ... M_{|\mipsset|}\}$ where
  $M_i$ is a power of 2.  Every $C_i \in \corenumset$ and GPU in this
  experiment has a MIPS count of $M_i$ where $M_i$ is sampled randomly
  from the set \mipsset. The elements of set \mipsset are chosen from a
  normal distribution ranging from: 100 to 1000000.

\end{enumerate}

For given values of \numtplgynodes, \gpunum, \veclenset, \corenumset,
and \mipsset and a given application, let the $k$-th \ul{trial} be
defined as one execution of the following sequence of steps.

\begin{itemize}

\item For each GPU $G_i$, sample \veclenset and \mipsset randomly to
  determine its vector length $V_i$ and MIPS count $M_i$. \label{i1}

\item For each CPU $P_i$, sample \corenumset randomly to determine the
  number of cores $C_i$ in the processor $P_i$.~\label{i2}

\item For each core $C_i$ in the processor $P_i$ sample $V_i$ and $M_i$
  randomly from set \veclenset and \mipsset.

\item Use our framework to extract data and task parallelism that is
  best utilizable by the heterogeneity created by parameters in items 1,
  2, and 3 above. Determine the execution time
  $L^{\mathcal{M}}$.

\end{itemize}

An experiment, \expt(\numtplgynodes, \gpunum, \veclenset, \corenumset
\mipsset), consists of conducting enough of the above trials so that
width of the 95\% confidence interval on the average value of
$Latency^{\zeta_\mathcal{M}}$ is less than 10\% of the average
value. This results in a variable number of trials with different
experimental set-ups. Note that two trials differ from each other only in
the seed for the random number generator.  This reduces the dependence
of our results on a lucky sequence of numbers from the random number
generator.

\subsubsection{The task graph set-up}
\label{sec:task-graph-setup}

We chose 5 applications from the HPC arena: binomial option pricing (a
financial derivatives application), 2-dimensional convolution (for image
processing), Gram Schmidt linear algebra kernel, 2-dimensional
Gauss-Seidel stencil computations, and finally our motivating example
itself the 2-dimensional Jacobi stencil computation. Next for these 5
applications, we varied the vector strip from 10 to 50, which resulted
in graphs varying from around 50 to 5000 nodes and with 23 to 12,000
edges. For example, given a task node with a vector length requirement
($T^i_1$) of 30,000 elements, a vector strip of 10 means dividing the
total vector requirement by 10, which results in 10 nodes each requiring
3000 vector elements. Similarly, the LLVM instruction count ($T^i_0$)
for each node varies depending upon the application at hand.

A detailed description of the applications and their features is shown
in Table~\ref{tab:1}. In general the vector requirement of the
applications in our benchmark suite varies from 1000 to 1 Million
elements. The LLVM instruction count varies from around 1 to 0.3
billion. The edge weights depicting the amount of data transfer on the
other hand varies from 3000 bytes to almost 4.8 Mega byte.

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Application} & \textbf{Vector strip} & $|V_t|$ & $|E_t|$ \\
    \hline
    \multirow{5}{*}{Binomial option pricing} & 10 & 82 & 206 \\
    & 20 & 102 & 306 \\
    & 30 & 122 & 406 \\
    & 40 & 142 & 506 \\
    & 50 & 162 & 606 \\
    \hline
    \multirow{5}{*}{Convolution} & 10 & 79 & 143 \\
    & 20 & 89 & 173 \\
    & 30 & 99 & 203 \\
    & 40 & 109 & 233 \\
    & 50 & 119 & 263 \\
    \hline
    \multirow{5}{*}{Gram Schmidt} & 10 & 228 & 443 \\
    & 20 & 838 & 1653 \\
    & 30 & 1848 & 3663 \\
    & 40 & 3258 & 6473 \\
    & 50 & 5068 & 10083\\
    \hline
    \multirow{5}{*}{Gauss-Seidel} & 10 & 227 & 531 \\
    & 20 & 837 & 2041 \\
    & 30 & 1847 & 4551 \\
    & 40 & 3257 & 8061 \\
    & 50 & 5067 & 12571\\
    \hline
    \multirow{5}{*}{Jacobi} & 10 & 48 & 130 \\
    & 20 & 78 & 240 \\
    & 30 & 108 & 350 \\
    & 40 & 138 & 460 \\
    & 50 & 168 & 570\\
    \hline
  \end{tabular}
  \caption{The task graph set-up}
  \label{tab:1}
\end{table}

% We have done experiments for several different sets of parameters. In
% this paper we show the results for the parameters enumerated in Table
% \ref{parametr_tbl}.

\begin{figure*}[t!]
  \centering
  \subfigure[Binomial Option Pricing]{
    \includegraphics[angle=0, scale=0.72]{./figures/bin_surface}
    \label{fig:bin1ho}
  }
  \subfigure[2 Dimensional Convolution]{
    \includegraphics[angle=0, scale=0.72]{./figures/conv_surface}
    \label{fig:conv1ho}
  }
  \subfigure[Gram Schmidt linear-algebra kernel]{
    \includegraphics[angle=0, scale=0.72]{./figures/gram_surface}
    \label{fig:gram1ho}
  }
  \subfigure[2 Dimensional Seidel stencil computation]{
    \includegraphics[angle=0, scale=0.72]{./figures/sei_surface}
    \label{fig:sei1ho}
  }
  \subfigure[2 Dimensional Jacobi stencil computation]{
    \includegraphics[angle=0, scale=0.72]{./figures/jac_surface}
    \label{fig:jacl1ho}
  }
  \caption{Comparison of execution times of ``Our Framework" and
    ``Heterogeneous Bin Packing"}
  \label{fig:ho}
\end{figure*}

\begin{figure*}[t!]
  \centering
  \subfigure[Binomial Option Pricing]{
    \includegraphics[angle=0, scale=0.65]{./figures/bin_32-64_surface}
    \label{fig:bin1comm}
  }
  \subfigure[2 Dimensional Jacobi stencil computation]{
    \includegraphics[angle=0, scale=0.65]{./figures/jac_32-64_surface}
    \label{fig:jac1comm}
  }
  \caption{Choosing the proper tile size}
  \label{fig:comm}
\end{figure*}

\subsection{Experimental Results}
\label{sec:results-1}

The experimental set-up consists of a dual socket system consisting of
Intel Xeon E5620 CPU running at 2.4Ghz with 24GB DDR3 RAM. The system
is running Linux kernel ver 3.0.40-1. Our framework was compiled
using gcc version 4.7.1 with '-O3' optimization flag.

The results for the experiments with the above experimental set-up
comparing our approach with the aforementioned heterogeneous bin packing
approach is shown in Figure~\ref{fig:ho}. Our approach performs better
compared to heterogeneous bin packing for all the applications. The
graphs in Figure~\ref{fig:ho} consists of two surfaces layered on top of
each other. In these graphs the larger the value, the worse the latency
and hence the performance of the application. % For all graphs, the
% surface representing the heterogeneous bin-packing solution is always
% layered atop our results, which clearly show the superiority of our
% approach. Note that the z-axis in all the graph in Figure~\ref{fig:ho}
% are in logarithmic scale with a base of 10. This in turn implies that
% our results are almost an order of magnitude better than those
% obtained by the heterogeneous bin packing heuristic.

For Seidel, the results for bin-packing could not be obtained, because
the required vector element length was larger than the available
capability of the underlying hardware. Our approach is able to deal with
such a situation, by fitting the required vector length to the
capability and then running the rest in an iterative manner.

The major differences between the two algorithms are provided in
Table~\ref{tab:2}. The \textbf{Max latency} and \textbf{Min latency}
columns give the maximum and the minimum latencies for bin packing and
our framework, respectively. The final column gives the number of data
points that our framework is better compared to heterogeneous bin
packing. As we can see from Table~\ref{tab:2}, our framework out
performs the heterogeneous bin-packing on all the applications.

\begin{table*}[t!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \textbf{Application} &
    \multicolumn{2}{|c|}{\textbf{Max latency (sec)}} &
    \multicolumn{2}{|c|}{\textbf{Min latency (sec)}} &
    \textbf{Better (\%)} \\
    \hline
    & {HBP} & {Our framework} &
    {HBP} & {Our framework} & HBP vs Our framework\\
    \hline
    Binomial option pricing & $1.35e+8$ & $5.27e+7$ & 77356.9 & 1 & 100 \\
    \hline
    Convolution & 489065 & 78.52 & 53.64 & 16.26 & 94\\
    \hline
    Gram Schmidt & 18984.5 & 177.97 & 2947.65 & 1.20 & 94\\
    \hline
    Gauss-Seidel & N/A & 3635.62 & N/A & 9.67 & 100\\
    \hline
    Jacobi & 13919.2 & 39.89 & 1.69 & 0.97 & 92\\
    \hline
  \end{tabular}
  \caption{Major statistics comparing heterogeneous bin packing and our framework}
  \label{tab:2}
\end{table*}

There are a number of reasons for the large differences seen in
Figure~\ref{fig:ho}.

\begin{itemize}

\item The bin packing solution gives priority to volume, which means
  vectors are packed into a single large vector processor, while giving
  very little consideration to the instruction count requirement.

\item The bin packing heuristic does not consider placing data-stores in
  the correct locations, in fact data-stores do not play a part in the
  whole process at all and the same can be said about communication.

\end{itemize}

Since communication does not play a part in the heuristic bin-packing
solution, we ignored the communication values when comparing with bin
packing. However, in order to show how communication can have a significant
effect onto the application design and parallelism potential, in the next
section we perform design space exploration with the effect
of communication as the main objective.

\subsection{Affects of varying communication}
\label{sec:arch-expl-with}

% In the previous section we compared our results with heterogeneous bin
% packing, without any regards for the communication. This was done by
% keeping the bandwidth of the resource graph links high enough to not
% have any affect on the overall application latency. We purposely did
% this, in order to make a fair comparison with bin packing since it does
% not consider communication when partitioning the nodes in the task
% graph.

In this section we vary the bandwidth (weights on the resource graph
edges, $E^C$) to explore best vector tile sizes for the applications.
For experimental purposes, we use a 2D tiled mesh of 16 $\times$ 16 PEs
(total of 256 PEs). The bandwidth of the links is randomly selected from
a normal distribution between 32-64 MB/s for each edge. In this set-up,
a GPU-CPU combination is placed at every fourth corner in the tiled
mesh. Finally, every CPU core in this set-up runs at 2GHz and has a
vector capacity of 64, i.e., 64 floating point elements can be processed
at once. Similarly, all GPUs run at 700MHz and can perform 4096 floating
point operations in a SIMD fashion. The application set is varied in the
exact same manner as the previous experimental set-up.


Two of the most interesting results that we obtained for the above
experimental set-up is shown in
Figure~\ref{fig:comm}. Figure~\ref{fig:bin1comm}, gives the varying
latency of execution for the different vector strip sizes for the
binomial option pricing application. As we can see, with increasing
vector strip sizes the latency of execution reduces. In our experiments
we found that a vector strip size of 50 is best suited for this example
and gave the best performance. Moreover, we also found that from the 256
PEs, this example only utilized 46 PEs and that too mostly GPUs.  Jacobi
on the other hand performs best for a vector strip size of
20. Increasing the vector strip size after that has an adverse affect on
the resulting partitioning as the communication overheads are far
greater than the computation potential. Finally, in both these
applications the placement of the data stores plays an important role,
since there is a single data store, which needs to be accessed by all
the nodes split due to vector tiling, the communication overhead
increases if the store is not placed on a correct PE: usually a PE with
high bandwidth connection to the PE processing the vector statement.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "bare_conf"
%%% End:

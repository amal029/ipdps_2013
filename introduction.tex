\section{Introduction}
\label{sec:introduction}

High performance computing (HPC) clusters increasingly consist of large numbers
of heterogeneous processing elements such as CPUs, graphics processing units
(GPUs), field programmable gate arrays (FPGAs), low-power processors intended
for digital signal processing (DSP), etc. By combining heterogeneous
processing units it may be possible to divide the work so that different
types of computation in the application are run on different types of
units. This can result in significant speed-ups, lower hardware costs
and/or reduced power consumption by the HPC system.  For example, if a
computation contains the right patterns of data parallelism it may run
dozens or even hundreds of times faster on a GPU than on a CPU that has
similar cost and power consumption. On the other hand, computations with
less data parallelism and more complex control flow may run faster on
CPUs. Matching the type of computation to the processor can yield
significant benefits. Although the potential of heterogeneous computing is
great, exploiting that potential is more difficult.


\vspace{1cm}


In this paper we consider the streaming~\cite{jbuck94} model of
computation. Streaming is a popular model for programs such as image and
signal processing, financial applications, networking,
telecommunications, etc. In the streaming model statements (also called
filters/actors/tasks or kernels) execute iteratively, processing the incoming
tokens of data. Given such a stream application, it is
difficult to partition the available parallelism onto the hardware. For
example, how does one decide which parallel filters should run on which
type of execution unit? Given a system with dozens or hundreds of CPUs,
GPUs and other units, how does one divide the work between them?  There
are several conflicting factors. For example, one wants to allocate
filters to the type of execution unit that will execute it most
efficiently. On the other hand, one wants to achieve a good load balance
by dividing the work evenly across the units.We want to allocate the filters to
reduce communication costs while at the same time taking account of all the
other factors.

We consider the problem of partitioning stream graphs onto
heterogeneous HPC computing systems. This problem has been
studied extensively for homogeneous architectures where all processing
elements are the same. Although the homogeneous case is
NP-hard~\cite{vsar89}, several heuristic solutions have been found that
work well in practice. However, extending these solutions to the
heterogeneous case is difficult for two reasons.

In the heterogeneous case some processing elements are more powerful
than others, so achieving a good load balance usually involves
distributing the work unevenly.

A second reason why it can be difficult to extend algorithms for
homogeneous architectures to the heterogeneous hardware relates to the
strengths and weaknesses of different types of processors. When
considering heterogeneous architectures, it is tempting to think of
some processing elements simply being more powerful than others.
A GPU is not simply a more powerful CPU. In fact, some types of
computation run better on CPUs and some on GPUs.  For a partitioning
algorithm to work well, it needs to take account of the strengths and
weaknesses of different types of processing elements. In this paper we
present an approach to partitioning parallel tasks to heterogeneous
architectures that addresses both of these concerns.

\vspace{0.2cm}

Our \textbf{main contributions} are as follows:
\begin{itemize}
\item We present a novel approach to characterizing the type of
  processing elements based on their level of vector parallelism which,
  allows us to distinguish the suitability of different types of units
  to different filters.
\item We provide a novel algorithm for partitioning task and data parallelism
  to heterogeneous architectures based on hierarchical graph
  partitioning.
\end{itemize}

The rest of this paper is organized as follows.
Section~\ref{sec:preliminaries} formalizes the problem statement and
defines the objective function. Next, in
Section~\ref{sec:our-framework}, we provide a detailed description of
our framework. Section~\ref{sec:experiments-results} gives the quantitative
comparisons of our approach against other
approaches. Section~\ref{sec:related-work} describes the related work
and positions our approach in comparison to these works.  Finally, we
conclude in Section~\ref{sec:conclusion}.
